api: torch.linalg.lu_factor
doc: "\n\ntorch.linalg.lu_factor(A, *, bool pivot=True, out=None) -> (Tensor, Tensor)\xB6\
  \nComputes a compact representation of the LU factorization with partial pivoting\
  \ of a matrix.\nThis function computes a compact representation of the decomposition\
  \ given by torch.linalg.lu().\nIf the matrix is square, this representation may\
  \ be used in torch.linalg.lu_solve()\nto solve system of linear equations that share\
  \ the matrix A.\nThe returned decomposition is represented as a named tuple (LU,\
  \ pivots).\nThe LU matrix has the same shape as the input matrix A. Its upper and\
  \ lower triangular\nparts encode the non-constant elements of L and U of the LU\
  \ decomposition of A.\nThe returned permutation matrix is represented by a 1-indexed\
  \ vector. pivots[i] == j represents\nthat in the i-th step of the algorithm, the\
  \ i-th row was permuted with the j-1-th row.\nOn CUDA, one may use pivot= False.\
  \ In this case, this function returns the LU\ndecomposition without pivoting if\
  \ it exists.\nSupports inputs of float, double, cfloat and cdouble dtypes.\nAlso\
  \ supports batches of matrices, and if the inputs are batches of matrices then\n\
  the output has the same batch dimensions.\n\nNote\nWhen inputs are on a CUDA device,\
  \ this function synchronizes that device with the CPU. For a version of this function\
  \ that does not synchronize, see torch.linalg.lu_factor_ex().\n\n\nWarning\nThe\
  \ LU decomposition is almost never unique, as often there are different permutation\n\
  matrices that can yield different LU decompositions.\nAs such, different platforms,\
  \ like SciPy, or inputs on different devices,\nmay produce different valid decompositions.\n\
  Gradient computations are only supported if the input matrix is full-rank.\nIf this\
  \ condition is not met, no error will be thrown, but the gradient may not be finite.\n\
  This is because the LU decomposition with pivoting is not differentiable at these\
  \ points.\n\n\nSee also\ntorch.linalg.lu_solve() solves a system of linear equations\
  \ given the output of this\nfunction provided the input matrix was square and invertible.\n\
  torch.lu_unpack() unpacks the tensors returned by lu_factor() into the three\nmatrices\
  \ P, L, U that form the decomposition.\ntorch.linalg.lu() computes the LU decomposition\
  \ with partial pivoting of a possibly\nnon-square matrix. It is a composition of\
  \ lu_factor() and torch.lu_unpack().\ntorch.linalg.solve() solves a system of linear\
  \ equations. It is a composition\nof lu_factor() and lu_solve().\n\n\nParameters\n\
  A (Tensor) \u2013 tensor of shape (*, m, n) where * is zero or more batch dimensions.\n\
  \nKeyword Arguments\n\npivot (bool, optional) \u2013 Whether to compute the LU decomposition\
  \ with partial pivoting, or the regular LU\ndecomposition. pivot= False not supported\
  \ on CPU. Default: True.\nout (tuple, optional) \u2013 tuple of two tensors to write\
  \ the output to. Ignored if None. Default: None.\n\n\nReturns\nA named tuple (LU,\
  \ pivots).\n\nRaises\nRuntimeError \u2013 if the A matrix is not invertible or any\
  \ matrix in a batched A\n    is not invertible.\n\n\nExamples:\n>>> A = torch.randn(2,\
  \ 3, 3)\n>>> B1 = torch.randn(2, 3, 4)\n>>> B2 = torch.randn(2, 3, 7)\n>>> LU, pivots\
  \ = torch.linalg.lu_factor(A)\n>>> X1 = torch.linalg.lu_solve(LU, pivots, B1)\n\
  >>> X2 = torch.linalg.lu_solve(LU, pivots, B2)\n>>> torch.allclose(A @ X1, B1)\n\
  True\n>>> torch.allclose(A @ X2, B2)\nTrue\n\n\n"
