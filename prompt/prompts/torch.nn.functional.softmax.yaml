api: torch.nn.functional.softmax
doc: "\n\ntorch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)[source]\xB6\
  \nApply a softmax function.\nSoftmax is defined as:\nSoftmax(xi)=exp\u2061(xi)\u2211\
  jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}Softmax(xi\u200B\
  )=\u2211j\u200Bexp(xj\u200B)exp(xi\u200B)\u200B\nIt is applied to all slices along\
  \ dim, and will re-scale them so that the elements\nlie in the range [0, 1] and\
  \ sum to 1.\nSee Softmax for more details.\n\nParameters\n\ninput (Tensor) \u2013\
  \ input\ndim (int) \u2013 A dimension along which softmax will be computed.\ndtype\
  \ (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified,\
  \ the input tensor is casted to dtype before the operation\nis performed. This is\
  \ useful for preventing data type overflows. Default: None.\n\n\nReturn type\nTensor\n\
  \n\n\nNote\nThis function doesn\u2019t work directly with NLLLoss,\nwhich expects\
  \ the Log to be computed between the Softmax and itself.\nUse log_softmax instead\
  \ (it\u2019s faster and has better numerical properties).\n\n"
