api: torch.fft.ifftn
doc: "\n\ntorch.fft.ifftn(input, s=None, dim=None, norm=None, *, out=None) \u2192\
  \ Tensor\xB6\nComputes the N dimensional inverse discrete Fourier transform of input.\n\
  \nNote\nSupports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or\
  \ greater.\nHowever it only supports powers of 2 signal length in every transformed\
  \ dimensions.\n\n\nParameters\n\ninput (Tensor) \u2013 the input tensor\ns (Tuple[int],\
  \ optional) \u2013 Signal size in the transformed dimensions.\nIf given, each dimension\
  \ dim[i] will either be zero-padded or\ntrimmed to the length s[i] before computing\
  \ the IFFT.\nIf a length -1 is specified, no padding is done in that dimension.\n\
  Default: s = [input.size(d) for d in dim]\ndim (Tuple[int], optional) \u2013 Dimensions\
  \ to be transformed.\nDefault: all dimensions, or the last len(s) dimensions if\
  \ s is given.\nnorm (str, optional) \u2013 Normalization mode. For the backward\
  \ transform\n(ifftn()), these correspond to:\n\n\"forward\" - no normalization\n\
  \"backward\" - normalize by 1/n\n\"ortho\" - normalize by 1/sqrt(n) (making the\
  \ IFFT orthonormal)\n\nWhere n = prod(s) is the logical IFFT size.\nCalling the\
  \ forward transform (fftn()) with the same\nnormalization mode will apply an overall\
  \ normalization of 1/n between\nthe two transforms. This is required to make ifftn()\n\
  the exact inverse.\nDefault is \"backward\" (normalize by 1/n).\n\n\n\nKeyword Arguments\n\
  out (Tensor, optional) \u2013 the output tensor.\n\n\nExample\n>>> x = torch.rand(10,\
  \ 10, dtype=torch.complex64)\n>>> ifftn = torch.fft.ifftn(x)\n\n\nThe discrete Fourier\
  \ transform is separable, so ifftn()\nhere is equivalent to two one-dimensional\
  \ ifft() calls:\n>>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n\
  >>> torch.testing.assert_close(ifftn, two_iffts, check_stride=False)\n\n\n"
