api: torch.linalg.eig
doc: "\n\ntorch.linalg.eig(A, *, out=None)\xB6\nComputes the eigenvalue decomposition\
  \ of a square matrix if it exists.\nLetting K\\mathbb{K}K be R\\mathbb{R}R or C\\\
  mathbb{C}C,\nthe eigenvalue decomposition of a square matrix\nA\u2208Kn\xD7nA \\\
  in \\mathbb{K}^{n \\times n}A\u2208Kn\xD7n (if it exists) is defined as\n\nA=Vdiag\u2061\
  (\u039B)V\u22121V\u2208Cn\xD7n,\u039B\u2208CnA = V \\operatorname{diag}(\\Lambda)\
  \ V^{-1}\\mathrlap{\\qquad V \\in \\mathbb{C}^{n \\times n}, \\Lambda \\in \\mathbb{C}^n}A=Vdiag(\u039B\
  )V\u22121V\u2208Cn\xD7n,\u039B\u2208CnThis decomposition exists if and only if AAA\
  \ is diagonalizable.\nThis is the case when all its eigenvalues are different.\n\
  Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches\
  \ of matrices, and if A is a batch of matrices then\nthe output has the same batch\
  \ dimensions.\nThe returned eigenvalues are not guaranteed to be in any specific\
  \ order.\n\nNote\nThe eigenvalues and eigenvectors of a real matrix may be complex.\n\
  \n\nNote\nWhen inputs are on a CUDA device, this function synchronizes that device\
  \ with the CPU.\n\n\nWarning\nThis function assumes that A is diagonalizable (for\
  \ example, when all the\neigenvalues are different). If it is not diagonalizable,\
  \ the returned\neigenvalues will be correct but A\u2260Vdiag\u2061(\u039B)V\u2212\
  1A \\neq V \\operatorname{diag}(\\Lambda)V^{-1}A\uE020=Vdiag(\u039B)V\u22121.\n\n\
  \nWarning\nThe returned eigenvectors are normalized to have norm 1.\nEven then,\
  \ the eigenvectors of a matrix are not unique, nor are they continuous with respect\
  \ to\nA. Due to this lack of uniqueness, different hardware and software may compute\n\
  different eigenvectors.\nThis non-uniqueness is caused by the fact that multiplying\
  \ an eigenvector by\nby ei\u03D5,\u03D5\u2208Re^{i \\phi}, \\phi \\in \\mathbb{R}ei\u03D5\
  ,\u03D5\u2208R produces another set of valid eigenvectors\nof the matrix.  For this\
  \ reason, the loss function shall not depend on the phase of the\neigenvectors,\
  \ as this quantity is not well-defined.\nThis is checked when computing the gradients\
  \ of this function. As such,\nwhen inputs are on a CUDA device, the computation\
  \ of the gradients\nof this function synchronizes that device with the CPU.\n\n\n\
  Warning\nGradients computed using the eigenvectors tensor will only be finite when\n\
  A has distinct eigenvalues.\nFurthermore, if the distance between any two eigenvalues\
  \ is close to zero,\nthe gradient will be numerically unstable, as it depends on\
  \ the eigenvalues\n\u03BBi\\lambda_i\u03BBi\u200B through the computation of\n1min\u2061\
  i\u2260j\u03BBi\u2212\u03BBj\\frac{1}{\\min_{i \\neq j} \\lambda_i - \\lambda_j}mini\uE020\
  =j\u200B\u03BBi\u200B\u2212\u03BBj\u200B1\u200B.\n\n\nSee also\ntorch.linalg.eigvals()\
  \ computes only the eigenvalues.\nUnlike torch.linalg.eig(), the gradients of eigvals()\
  \ are always\nnumerically stable.\ntorch.linalg.eigh() for a (faster) function that\
  \ computes the eigenvalue decomposition\nfor Hermitian and symmetric matrices.\n\
  torch.linalg.svd() for a function that computes another type of spectral\ndecomposition\
  \ that works on matrices of any shape.\ntorch.linalg.qr() for another (much faster)\
  \ decomposition that works on matrices of\nany shape.\n\n\nParameters\nA (Tensor)\
  \ \u2013 tensor of shape (*, n, n) where * is zero or more batch dimensions\nconsisting\
  \ of diagonalizable matrices.\n\nKeyword Arguments\nout (tuple, optional) \u2013\
  \ output tuple of two tensors. Ignored if None. Default: None.\n\nReturns\nA named\
  \ tuple (eigenvalues, eigenvectors) which corresponds to \u039B\\Lambda\u039B and\
  \ VVV above.\neigenvalues and eigenvectors will always be complex-valued, even when\
  \ A is real. The eigenvectors\nwill be given by the columns of eigenvectors.\n\n\
  \n\nExamples:\n>>> A = torch.randn(2, 2, dtype=torch.complex128)\n>>> A\ntensor([[\
  \ 0.9828+0.3889j, -0.4617+0.3010j],\n        [ 0.1662-0.7435j, -0.6139+0.0562j]],\
  \ dtype=torch.complex128)\n>>> L, V = torch.linalg.eig(A)\n>>> L\ntensor([ 1.1226+0.5738j,\
  \ -0.7537-0.1286j], dtype=torch.complex128)\n>>> V\ntensor([[ 0.9218+0.0000j,  0.1882-0.2220j],\n\
  \        [-0.0270-0.3867j,  0.9567+0.0000j]], dtype=torch.complex128)\n>>> torch.dist(V\
  \ @ torch.diag(L) @ torch.linalg.inv(V), A)\ntensor(7.7119e-16, dtype=torch.float64)\n\
  \n>>> A = torch.randn(3, 2, 2, dtype=torch.float64)\n>>> L, V = torch.linalg.eig(A)\n\
  >>> torch.dist(V @ torch.diag_embed(L) @ torch.linalg.inv(V), A)\ntensor(3.2841e-16,\
  \ dtype=torch.float64)\n\n\n"
