api: torch.nn.functional.feature_alpha_dropout
doc: "\n\ntorch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False,\
  \ inplace=False)[source]\xB6\nRandomly masks out entire channels (a channel is a\
  \ feature map).\nFor example, the jjj-th channel of the iii-th sample in the batch\
  \ input\nis a tensor input[i,j]\\text{input}[i, j]input[i,j] of the input tensor.\
  \ Instead of\nsetting activations to zero, as in regular Dropout, the activations\
  \ are set\nto the negative saturation value of the SELU activation function.\nEach\
  \ element will be masked independently on every forward call with\nprobability p\
  \ using samples from a Bernoulli distribution.\nThe elements to be masked are randomized\
  \ on every forward call, and scaled\nand shifted to maintain zero mean and unit\
  \ variance.\nSee FeatureAlphaDropout for details.\n\nParameters\n\np (float) \u2013\
  \ dropout probability of a channel to be zeroed. Default: 0.5\ntraining (bool) \u2013\
  \ apply dropout if is True. Default: True\ninplace (bool) \u2013 If set to True,\
  \ will do this operation in-place. Default: False\n\n\nReturn type\nTensor\n\n\n"
