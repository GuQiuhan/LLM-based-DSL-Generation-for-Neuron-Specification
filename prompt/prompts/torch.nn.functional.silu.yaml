api: torch.nn.functional.silu
doc: "\n\ntorch.nn.functional.silu(input, inplace=False)[source]\xB6\nApply the Sigmoid\
  \ Linear Unit (SiLU) function, element-wise.\nThe SiLU function is also known as\
  \ the swish function.\n\nsilu(x)=x\u2217\u03C3(x),where\_\u03C3(x)\_is\_the\_logistic\_\
  sigmoid.\\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the\
  \ logistic sigmoid.}\n\nsilu(x)=x\u2217\u03C3(x),where\_\u03C3(x)\_is\_the\_logistic\_\
  sigmoid.\nNote\nSee Gaussian Error Linear Units (GELUs)\nwhere the SiLU (Sigmoid\
  \ Linear Unit) was originally coined, and see\nSigmoid-Weighted Linear Units for\
  \ Neural Network Function Approximation\nin Reinforcement Learning and Swish:\n\
  a Self-Gated Activation Function\nwhere the SiLU was experimented with later.\n\n\
  See SiLU for more details.\n\nReturn type\nTensor\n\n\n"
