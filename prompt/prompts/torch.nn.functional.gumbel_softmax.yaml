api: torch.nn.functional.gumbel_softmax
doc: "\n\ntorch.nn.functional.gumbel_softmax(logits, tau=1, hard=False, eps=1e-10,\
  \ dim=-1)[source]\xB6\nSample from the Gumbel-Softmax distribution (Link 1 Link\
  \ 2) and optionally discretize.\n\nParameters\n\nlogits (Tensor) \u2013 [\u2026\
  , num_features] unnormalized log probabilities\ntau (float) \u2013 non-negative\
  \ scalar temperature\nhard (bool) \u2013 if True, the returned samples will be discretized\
  \ as one-hot vectors,\nbut will be differentiated as if it is the soft sample in\
  \ autograd\ndim (int) \u2013 A dimension along which softmax will be computed. Default:\
  \ -1.\n\n\nReturns\nSampled tensor of same shape as logits from the Gumbel-Softmax\
  \ distribution.\nIf hard=True, the returned samples will be one-hot, otherwise they\
  \ will\nbe probability distributions that sum to 1 across dim.\n\nReturn type\n\
  Tensor\n\n\n\nNote\nThis function is here for legacy reasons, may be removed from\
  \ nn.Functional in the future.\n\n\nNote\nThe main trick for hard is to do  y_hard\
  \ - y_soft.detach() + y_soft\nIt achieves two things:\n- makes the output value\
  \ exactly one-hot\n(since we add then subtract y_soft value)\n- makes the gradient\
  \ equal to y_soft gradient\n(since we strip all other gradients)\n\n\nExamples::>>>\
  \ logits = torch.randn(20, 32)\n>>> # Sample soft categorical using reparametrization\
  \ trick:\n>>> F.gumbel_softmax(logits, tau=1, hard=False)\n>>> # Sample hard categorical\
  \ using \"Straight-through\" trick:\n>>> F.gumbel_softmax(logits, tau=1, hard=True)\n\
  \n\n\n\n"
