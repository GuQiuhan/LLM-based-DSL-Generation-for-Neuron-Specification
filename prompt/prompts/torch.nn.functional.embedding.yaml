api: torch.nn.functional.embedding
doc: "\n\ntorch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None,\
  \ norm_type=2.0, scale_grad_by_freq=False, sparse=False)[source]\xB6\nGenerate a\
  \ simple lookup table that looks up embeddings in a fixed dictionary and size.\n\
  This module is often used to retrieve word embeddings using indices.\nThe input\
  \ to the module is a list of indices, and the embedding matrix,\nand the output\
  \ is the corresponding word embeddings.\nSee torch.nn.Embedding for more details.\n\
  \nNote\nNote that the analytical gradients of this function with respect to\nentries\
  \ in weight at the row specified by padding_idx\nare expected to differ from the\
  \ numerical ones.\n\n\nNote\nNote that :class:`torch.nn.Embedding differs from this\
  \ function in\nthat it initializes the row of weight specified by\npadding_idx to\
  \ all zeros on construction.\n\n\nParameters\n\ninput (LongTensor) \u2013 Tensor\
  \ containing indices into the embedding matrix\nweight (Tensor) \u2013 The embedding\
  \ matrix with number of rows equal to the maximum possible index + 1,\nand number\
  \ of columns equal to the embedding size\npadding_idx (int, optional) \u2013 If\
  \ specified, the entries at padding_idx do not contribute to the gradient;\ntherefore,\
  \ the embedding vector at padding_idx is not updated during training,\ni.e. it remains\
  \ as a fixed \u201Cpad\u201D.\nmax_norm (float, optional) \u2013 If given, each\
  \ embedding vector with norm larger than max_norm\nis renormalized to have norm\
  \ max_norm.\nNote: this will modify weight in-place.\nnorm_type (float, optional)\
  \ \u2013 The p of the p-norm to compute for the max_norm option. Default 2.\nscale_grad_by_freq\
  \ (bool, optional) \u2013 If given, this will scale gradients by the inverse of\
  \ frequency of\nthe words in the mini-batch. Default False.\nsparse (bool, optional)\
  \ \u2013 If True, gradient w.r.t. weight will be a sparse tensor. See Notes under\n\
  torch.nn.Embedding for more details regarding sparse gradients.\n\n\nReturn type\n\
  Tensor\n\n\n\nShape:\nInput: LongTensor of arbitrary shape containing the indices\
  \ to extract\nWeight: Embedding matrix of floating point type with shape (V, embedding_dim),\n\
  where V = maximum index + 1 and embedding_dim = the embedding size\nOutput: (*,\
  \ embedding_dim), where * is the input shape\n\n\n\nExamples:\n>>> # a batch of\
  \ 2 samples of 4 indices each\n>>> input = torch.tensor([[1, 2, 4, 5], [4, 3, 2,\
  \ 9]])\n>>> # an embedding matrix containing 10 tensors of size 3\n>>> embedding_matrix\
  \ = torch.rand(10, 3)\n>>> F.embedding(input, embedding_matrix)\ntensor([[[ 0.8490,\
  \  0.9625,  0.6753],\n         [ 0.9666,  0.7761,  0.6108],\n         [ 0.6246,\
  \  0.9751,  0.3618],\n         [ 0.4161,  0.2419,  0.7383]],\n\n        [[ 0.6246,\
  \  0.9751,  0.3618],\n         [ 0.0237,  0.7794,  0.0528],\n         [ 0.9666,\
  \  0.7761,  0.6108],\n         [ 0.3385,  0.8612,  0.1867]]])\n\n>>> # example with\
  \ padding_idx\n>>> weights = torch.rand(10, 3)\n>>> weights[0, :].zero_()\n>>> embedding_matrix\
  \ = weights\n>>> input = torch.tensor([[0, 2, 0, 5]])\n>>> F.embedding(input, embedding_matrix,\
  \ padding_idx=0)\ntensor([[[ 0.0000,  0.0000,  0.0000],\n         [ 0.5609,  0.5384,\
  \  0.8720],\n         [ 0.0000,  0.0000,  0.0000],\n         [ 0.6262,  0.2438,\
  \  0.7471]]])\n\n\n"
