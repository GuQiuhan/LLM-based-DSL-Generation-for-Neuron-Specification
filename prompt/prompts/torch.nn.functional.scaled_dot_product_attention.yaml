api: torch.nn.functional.scaled_dot_product_attention
doc: "\n\ntorch.nn.functional.scaled_dot_product_attention()\xB6\n\nscaled_dot_product_attention(query,\
  \ key, value, attn_mask=None, dropout_p=0.0,is_causal=False, scale=None, enable_gqa=False)\
  \ -> Tensor:\n\n\nComputes scaled dot product attention on query, key and value\
  \ tensors, using an optional attention mask if passed,\nand applying dropout if\
  \ a probability greater than 0.0 is specified. The optional scale argument can only\
  \ be\nspecified as a keyword argument.\n# Efficient implementation equivalent to\
  \ the following:\ndef scaled_dot_product_attention(query, key, value, attn_mask=None,\
  \ dropout_p=0.0,\n        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n\
  \    L, S = query.size(-2), key.size(-2)\n    scale_factor = 1 / math.sqrt(query.size(-1))\
  \ if scale is None else scale\n    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n\
  \    if is_causal:\n        assert attn_mask is None\n        temp_mask = torch.ones(L,\
  \ S, dtype=torch.bool).tril(diagonal=0)\n        attn_bias.masked_fill_(temp_mask.logical_not(),\
  \ float(\"-inf\"))\n        attn_bias.to(query.dtype)\n\n    if attn_mask is not\
  \ None:\n        if attn_mask.dtype == torch.bool:\n            attn_bias.masked_fill_(attn_mask.logical_not(),\
  \ float(\"-inf\"))\n        else:\n            attn_bias += attn_mask\n\n    if\
  \ enable_gqa:\n        key = key.repeat_interleave(query.size(-3)//key.size(-3),\
  \ -3)\n        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n\
  \n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n    attn_weight\
  \ += attn_bias\n    attn_weight = torch.softmax(attn_weight, dim=-1)\n    attn_weight\
  \ = torch.dropout(attn_weight, dropout_p, train=True)\n    return attn_weight @\
  \ value\n\n\n\nWarning\nThis function is beta and subject to change.\n\n\nWarning\n\
  This function always applies dropout according to the specified dropout_p argument.\n\
  To disable dropout during evaluation, be sure to pass a value of 0.0 when the module\n\
  that makes the function call is not in training mode.\nFor example:\nclass MyModel(nn.Module):\n\
  \    def __init__(self, p=0.5):\n        super().__init__()\n        self.p = p\n\
  \n    def forward(self, ...):\n        return F.scaled_dot_product_attention(...,\n\
  \            dropout_p=(self.p if self.training else 0.0))\n\n\n\n\nNote\nThere\
  \ are currently three supported implementations of scaled dot product attention:\n\
  \n\nFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\n\
  Memory-Efficient Attention\nA PyTorch implementation defined in C++ matching the\
  \ above formulation\n\n\nThe function may call optimized kernels for improved performance\
  \ when using the CUDA backend.\nFor all other backends, the PyTorch implementation\
  \ will be used.\nAll implementations are enabled by default. Scaled dot product\
  \ attention attempts to automatically select the\nmost optimal implementation based\
  \ on the inputs. In order to provide more fine-grained control over what implementation\n\
  is used, the following functions are provided for enabling and disabling implementations.\n\
  The context manager is the preferred mechanism:\n\n\ntorch.nn.attention.sdpa_kernel():\
  \ A context manager used to enable or disable any of the implementations.\ntorch.backends.cuda.enable_flash_sdp():\
  \ Globally enables or disables FlashAttention.\ntorch.backends.cuda.enable_mem_efficient_sdp():\
  \ Globally enables or disables  Memory-Efficient Attention.\ntorch.backends.cuda.enable_math_sdp():\
  \ Globally enables or disables  the PyTorch C++ implementation.\n\n\nEach of the\
  \ fused kernels has specific input limitations. If the user requires the use of\
  \ a specific fused implementation,\ndisable the PyTorch C++ implementation using\
  \ torch.nn.attention.sdpa_kernel().\nIn the event that a fused implementation is\
  \ not available, a warning will be raised with the\nreasons why the fused implementation\
  \ cannot run.\nDue to the nature of fusing floating point operations, the output\
  \ of this function may be different\ndepending on what backend kernel is chosen.\n\
  The c++ implementation supports torch.float64 and can be used when higher precision\
  \ is required.\nFor math backend, all intermediates are kept in torch.float if inputs\
  \ are in torch.half or torch.bfloat16.\n\nFor more information please see Numerical\
  \ accuracy\n\nGrouped Query Attention (GQA) is an experimental feature. It currently\
  \ works only for Flash_attention\nand math kernel on CUDA tensor, and does not support\
  \ Nested tensor.\nConstraints for GQA:\n\n\nnumber_of_heads_query % number_of_heads_key_value\
  \ == 0 and,\nnumber_of_heads_key == number_of_heads_value\n\n\n\n\nNote\nIn some\
  \ circumstances when given tensors on a CUDA device and using CuDNN, this operator\
  \ may select a nondeterministic algorithm to increase performance. If this is undesirable,\
  \ you can try to make the operation deterministic (potentially at a performance\
  \ cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility\
  \ for more information.\n\n\nParameters\n\nquery (Tensor) \u2013 Query tensor; shape\
  \ (N,...,Hq,L,E)(N, ..., Hq, L, E)(N,...,Hq,L,E).\nkey (Tensor) \u2013 Key tensor;\
  \ shape (N,...,H,S,E)(N, ..., H, S, E)(N,...,H,S,E).\nvalue (Tensor) \u2013 Value\
  \ tensor; shape (N,...,H,S,Ev)(N, ..., H, S, Ev)(N,...,H,S,Ev).\nattn_mask (optional\
  \ Tensor) \u2013 Attention mask; shape must be broadcastable to the shape of attention\
  \ weights,\nwhich is (N,...,L,S)(N,..., L, S)(N,...,L,S). Two types of masks are\
  \ supported.\nA boolean mask where a value of True indicates that the element should\
  \ take part in attention.\nA float mask of the same type as query, key, value that\
  \ is added to the attention score.\ndropout_p (float) \u2013 Dropout probability;\
  \ if greater than 0.0, dropout is applied\nis_causal (bool) \u2013 If set to true,\
  \ the attention masking is a lower triangular matrix when the mask is a\nsquare\
  \ matrix. The attention masking has the form of the upper left causal bias due to\
  \ the alignment\n(see torch.nn.attention.bias.CausalBias) when the mask is a non-square\
  \ matrix.\nAn error is thrown if both attn_mask and is_causal are set.\nscale (optional\
  \ python:float, keyword-only) \u2013 Scaling factor applied prior to softmax. If\
  \ None, the default value is set\nto 1E\\frac{1}{\\sqrt{E}}E\u200B1\u200B.\nenable_gqa\
  \ (bool) \u2013 If set to True, Grouped Query Attention (GQA) is enabled, by default\
  \ it is set to False.\n\n\nReturns\nAttention output; shape (N,...,Hq,L,Ev)(N, ...,\
  \ Hq, L, Ev)(N,...,Hq,L,Ev).\n\nReturn type\noutput (Tensor)\n\n\n\nShape legend:\n\
  N:Batch\_size...:Any\_number\_of\_other\_batch\_dimensions\_(optional)N: \\text{Batch\
  \ size} ... : \\text{Any number of other batch dimensions (optional)}N:Batch\_size...:Any\_\
  number\_of\_other\_batch\_dimensions\_(optional)\nS:Source\_sequence\_lengthS: \\\
  text{Source sequence length}S:Source\_sequence\_length\nL:Target\_sequence\_lengthL:\
  \ \\text{Target sequence length}L:Target\_sequence\_length\nE:Embedding\_dimension\_\
  of\_the\_query\_and\_keyE: \\text{Embedding dimension of the query and key}E:Embedding\_\
  dimension\_of\_the\_query\_and\_key\nEv:Embedding\_dimension\_of\_the\_valueEv:\
  \ \\text{Embedding dimension of the value}Ev:Embedding\_dimension\_of\_the\_value\n\
  Hq:Number\_of\_heads\_of\_queryHq: \\text{Number of heads of query}Hq:Number\_of\_\
  heads\_of\_query\nH:Number\_of\_heads\_of\_key\_and\_valueH: \\text{Number of heads\
  \ of key and value}H:Number\_of\_heads\_of\_key\_and\_value\n\n\n\nExamples\n>>>\
  \ # Optionally use the context manager to ensure one of the fused kernels is run\n\
  >>> query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>>\
  \ key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> value\
  \ = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> with sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION]):\n\
  >>>     F.scaled_dot_product_attention(query,key,value)\n\n\n>>> # Sample for GQA\
  \ for llama3\n>>> query = torch.rand(32, 32, 128, 64, dtype=torch.float16, device=\"\
  cuda\")\n>>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\"\
  )\n>>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n\
  >>> with sdpa_kernel(backends=[SDPBackend.MATH]):\n>>>     F.scaled_dot_product_attention(query,key,value,enable_gqa=True)\n\
  \n\n"
