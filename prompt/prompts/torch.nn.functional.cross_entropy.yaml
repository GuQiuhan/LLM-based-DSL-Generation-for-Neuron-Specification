api: torch.nn.functional.cross_entropy
doc: "\n\ntorch.nn.functional.cross_entropy(input, target, weight=None, size_average=None,\
  \ ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)[source]\xB6\
  \nCompute the cross entropy loss between input logits and target.\nSee CrossEntropyLoss\
  \ for details.\n\nParameters\n\ninput (Tensor) \u2013 Predicted unnormalized logits;\n\
  see Shape section below for supported shapes.\ntarget (Tensor) \u2013 Ground truth\
  \ class indices or class probabilities;\nsee Shape section below for supported shapes.\n\
  weight (Tensor, optional) \u2013 a manual rescaling weight given to each\nclass.\
  \ If given, has to be a Tensor of size C\nsize_average (bool, optional) \u2013 Deprecated\
  \ (see reduction). By default,\nthe losses are averaged over each loss element in\
  \ the batch. Note that for\nsome losses, there multiple elements per sample. If\
  \ the field size_average\nis set to False, the losses are instead summed for each\
  \ minibatch. Ignored\nwhen reduce is False. Default: True\nignore_index (int, optional)\
  \ \u2013 Specifies a target value that is ignored\nand does not contribute to the\
  \ input gradient. When size_average is\nTrue, the loss is averaged over non-ignored\
  \ targets. Note that\nignore_index is only applicable when the target contains class\
  \ indices.\nDefault: -100\nreduce (bool, optional) \u2013 Deprecated (see reduction).\
  \ By default, the\nlosses are averaged or summed over observations for each minibatch\
  \ depending\non size_average. When reduce is False, returns a loss per\nbatch element\
  \ instead and ignores size_average. Default: True\nreduction (str, optional) \u2013\
  \ Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'. 'none':\
  \ no reduction will be applied,\n'mean': the sum of the output will be divided by\
  \ the number of\nelements in the output, 'sum': the output will be summed. Note:\
  \ size_average\nand reduce are in the process of being deprecated, and in the meantime,\n\
  specifying either of those two args will override reduction. Default: 'mean'\nlabel_smoothing\
  \ (float, optional) \u2013 A float in [0.0, 1.0]. Specifies the amount\nof smoothing\
  \ when computing the loss, where 0.0 means no smoothing. The targets\nbecome a mixture\
  \ of the original ground truth and a uniform distribution as described in\nRethinking\
  \ the Inception Architecture for Computer Vision. Default: 0.00.00.0.\n\n\nReturn\
  \ type\nTensor\n\n\n\nShape:\nInput: Shape (C)(C)(C), (N,C)(N, C)(N,C) or (N,C,d1,d2,...,dK)(N,\
  \ C, d_1, d_2, ..., d_K)(N,C,d1\u200B,d2\u200B,...,dK\u200B) with K\u22651K \\geq\
  \ 1K\u22651\nin the case of K-dimensional loss.\nTarget: If containing class indices,\
  \ shape ()()(), (N)(N)(N) or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200B\
  ,d2\u200B,...,dK\u200B) with\nK\u22651K \\geq 1K\u22651 in the case of K-dimensional\
  \ loss where each value should be between [0,C)[0, C)[0,C).\nIf containing class\
  \ probabilities, same shape as the input and each value should be between [0,1][0,\
  \ 1][0,1].\n\nwhere:\n\nC=number\_of\_classesN=batch\_size\\begin{aligned}\n   \
  \ C ={} & \\text{number of classes} \\\\\n    N ={} & \\text{batch size} \\\\\n\\\
  end{aligned}\n\nC=N=\u200Bnumber\_of\_classesbatch\_size\u200B\n\nExamples:\n>>>\
  \ # Example of target with class indices\n>>> input = torch.randn(3, 5, requires_grad=True)\n\
  >>> target = torch.randint(5, (3,), dtype=torch.int64)\n>>> loss = F.cross_entropy(input,\
  \ target)\n>>> loss.backward()\n>>>\n>>> # Example of target with class probabilities\n\
  >>> input = torch.randn(3, 5, requires_grad=True)\n>>> target = torch.randn(3, 5).softmax(dim=1)\n\
  >>> loss = F.cross_entropy(input, target)\n>>> loss.backward()\n\n\n"
