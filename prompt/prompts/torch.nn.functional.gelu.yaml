api: torch.nn.functional.gelu
doc: "\n\ntorch.nn.functional.gelu(input, approximate='none') \u2192 Tensor\xB6\n\
  When the approximate argument is \u2018none\u2019, it applies element-wise the function\n\
  GELU(x)=x\u2217\u03A6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03A6(x)\n\
  where \u03A6(x)\\Phi(x)\u03A6(x) is the Cumulative Distribution Function for Gaussian\
  \ Distribution.\nWhen the approximate argument is \u2018tanh\u2019, Gelu is estimated\
  \ with\n\nGELU(x)=0.5\u2217x\u2217(1+Tanh(2/\u03C0\u2217(x+0.044715\u2217x3)))\\\
  text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt{2 / \\pi} * (x + 0.044715 * x^3)))\n\
  \nGELU(x)=0.5\u2217x\u2217(1+Tanh(2/\u03C0\u200B\u2217(x+0.044715\u2217x3)))See\
  \ Gaussian Error Linear Units (GELUs).\n"
