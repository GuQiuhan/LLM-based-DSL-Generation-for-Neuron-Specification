api: torch.nn.functional.ctc_loss
doc: "\n\ntorch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths,\
  \ blank=0, reduction='mean', zero_infinity=False)[source]\xB6\nApply the Connectionist\
  \ Temporal Classification loss.\nSee CTCLoss for details.\n\nNote\nIn some circumstances\
  \ when given tensors on a CUDA device and using CuDNN, this operator may select\
  \ a nondeterministic algorithm to increase performance. If this is undesirable,\
  \ you can try to make the operation deterministic (potentially at a performance\
  \ cost) by setting torch.backends.cudnn.deterministic = True. See Reproducibility\
  \ for more information.\n\n\nNote\nThis operation may produce nondeterministic gradients\
  \ when given tensors on a CUDA device. See Reproducibility for more information.\n\
  \n\nParameters\n\nlog_probs (Tensor) \u2013 (T,N,C)(T, N, C)(T,N,C) or (T,C)(T,\
  \ C)(T,C) where C = number of characters in alphabet including blank,\nT = input\
  \ length, and N = batch size.\nThe logarithmized probabilities of the outputs\n\
  (e.g. obtained with torch.nn.functional.log_softmax()).\ntargets (Tensor) \u2013\
  \ (N,S)(N, S)(N,S) or (sum(target_lengths)).\nTargets cannot be blank. In the second\
  \ form, the targets are assumed to be concatenated.\ninput_lengths (Tensor) \u2013\
  \ (N)(N)(N) or ()()().\nLengths of the inputs (must each be \u2264T\\leq T\u2264\
  T)\ntarget_lengths (Tensor) \u2013 (N)(N)(N) or ()()().\nLengths of the targets\n\
  blank (int, optional) \u2013 Blank label. Default 000.\nreduction (str, optional)\
  \ \u2013 Specifies the reduction to apply to the output:\n'none' | 'mean' | 'sum'.\
  \ 'none': no reduction will be applied,\n'mean': the output losses will be divided\
  \ by the target lengths and\nthen the mean over the batch is taken, 'sum': the output\
  \ will be\nsummed. Default: 'mean'\nzero_infinity (bool, optional) \u2013 Whether\
  \ to zero infinite losses and the associated gradients.\nDefault: False\nInfinite\
  \ losses mainly occur when the inputs are too short\nto be aligned to the targets.\n\
  \n\nReturn type\nTensor\n\n\nExample:\n>>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n\
  >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n>>> input_lengths\
  \ = torch.full((16,), 50, dtype=torch.long)\n>>> target_lengths = torch.randint(10,\
  \ 30, (16,), dtype=torch.long)\n>>> loss = F.ctc_loss(log_probs, targets, input_lengths,\
  \ target_lengths)\n>>> loss.backward()\n\n\n"
