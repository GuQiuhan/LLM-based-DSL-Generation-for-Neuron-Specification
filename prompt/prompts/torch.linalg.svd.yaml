api: torch.linalg.svd
doc: "\n\ntorch.linalg.svd(A, full_matrices=True, *, driver=None, out=None)\xB6\n\
  Computes the singular value decomposition (SVD) of a matrix.\nLetting K\\mathbb{K}K\
  \ be R\\mathbb{R}R or C\\mathbb{C}C,\nthe full SVD of a matrix\nA\u2208Km\xD7nA\
  \ \\in \\mathbb{K}^{m \\times n}A\u2208Km\xD7n, if k = min(m,n), is defined as\n\
  \nA=Udiag\u2061(S)VHU\u2208Km\xD7m,S\u2208Rk,V\u2208Kn\xD7nA = U \\operatorname{diag}(S)\
  \ V^{\\text{H}}\n\\mathrlap{\\qquad U \\in \\mathbb{K}^{m \\times m}, S \\in \\\
  mathbb{R}^k, V \\in \\mathbb{K}^{n \\times n}}A=Udiag(S)VHU\u2208Km\xD7m,S\u2208\
  Rk,V\u2208Kn\xD7nwhere diag\u2061(S)\u2208Km\xD7n\\operatorname{diag}(S) \\in \\\
  mathbb{K}^{m \\times n}diag(S)\u2208Km\xD7n,\nVHV^{\\text{H}}VH is the conjugate\
  \ transpose when VVV is complex, and the transpose when VVV is real-valued.\nThe\
  \ matrices  UUU, VVV (and thus VHV^{\\text{H}}VH) are orthogonal in the real case,\
  \ and unitary in the complex case.\nWhen m > n (resp. m < n) we can drop the last\
  \ m - n (resp. n - m) columns of U (resp. V) to form the reduced SVD:\n\nA=Udiag\u2061\
  (S)VHU\u2208Km\xD7k,S\u2208Rk,V\u2208Kk\xD7nA = U \\operatorname{diag}(S) V^{\\\
  text{H}}\n\\mathrlap{\\qquad U \\in \\mathbb{K}^{m \\times k}, S \\in \\mathbb{R}^k,\
  \ V \\in \\mathbb{K}^{k \\times n}}A=Udiag(S)VHU\u2208Km\xD7k,S\u2208Rk,V\u2208\
  Kk\xD7nwhere diag\u2061(S)\u2208Kk\xD7k\\operatorname{diag}(S) \\in \\mathbb{K}^{k\
  \ \\times k}diag(S)\u2208Kk\xD7k.\nIn this case, UUU and VVV also have orthonormal\
  \ columns.\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports\
  \ batches of matrices, and if A is a batch of matrices then\nthe output has the\
  \ same batch dimensions.\nThe returned decomposition is a named tuple (U, S, Vh)\n\
  which corresponds to UUU, SSS, VHV^{\\text{H}}VH above.\nThe singular values are\
  \ returned in descending order.\nThe parameter full_matrices chooses between the\
  \ full (default) and reduced SVD.\nThe driver kwarg may be used in CUDA with a cuSOLVER\
  \ backend to choose the algorithm used to compute the SVD.\nThe choice of a driver\
  \ is a trade-off between accuracy and speed.\n\nIf A is well-conditioned (its condition\
  \ number is not too large), or you do not mind some precision loss.\n\nFor a general\
  \ matrix: \u2018gesvdj\u2019 (Jacobi method)\nIf A is tall or wide (m >> n or m\
  \ << n): \u2018gesvda\u2019 (Approximate method)\n\n\nIf A is not well-conditioned\
  \ or precision is relevant: \u2018gesvd\u2019 (QR based)\n\nBy default (driver=\
  \ None), we call \u2018gesvdj\u2019 and, if it fails, we fallback to \u2018gesvd\u2019\
  .\nDifferences with numpy.linalg.svd:\n\nUnlike numpy.linalg.svd, this function\
  \ always returns a tuple of three tensors\nand it doesn\u2019t support compute_uv\
  \ argument.\nPlease use torch.linalg.svdvals(), which computes only the singular\
  \ values,\ninstead of compute_uv=False.\n\n\nNote\nWhen full_matrices= True, the\
  \ gradients with respect to U[\u2026, :, min(m, n):]\nand Vh[\u2026, min(m, n):,\
  \ :] will be ignored, as those vectors can be arbitrary bases\nof the corresponding\
  \ subspaces.\n\n\nWarning\nThe returned tensors U and V are not unique, nor are\
  \ they continuous with\nrespect to A.\nDue to this lack of uniqueness, different\
  \ hardware and software may compute\ndifferent singular vectors.\nThis non-uniqueness\
  \ is caused by the fact that multiplying any pair of singular\nvectors uk,vku_k,\
  \ v_kuk\u200B,vk\u200B by -1 in the real case or by\nei\u03D5,\u03D5\u2208Re^{i\
  \ \\phi}, \\phi \\in \\mathbb{R}ei\u03D5,\u03D5\u2208R in the complex case produces\
  \ another two\nvalid singular vectors of the matrix.\nFor this reason, the loss\
  \ function shall not depend on this ei\u03D5e^{i \\phi}ei\u03D5 quantity,\nas it\
  \ is not well-defined.\nThis is checked for complex inputs when computing the gradients\
  \ of this function. As such,\nwhen inputs are complex and are on a CUDA device,\
  \ the computation of the gradients\nof this function synchronizes that device with\
  \ the CPU.\n\n\nWarning\nGradients computed using U or Vh will only be finite when\n\
  A does not have repeated singular values. If A is rectangular,\nadditionally, zero\
  \ must also not be one of its singular values.\nFurthermore, if the distance between\
  \ any two singular values is close to zero,\nthe gradient will be numerically unstable,\
  \ as it depends on the singular values\n\u03C3i\\sigma_i\u03C3i\u200B through the\
  \ computation of\n1min\u2061i\u2260j\u03C3i2\u2212\u03C3j2\\frac{1}{\\min_{i \\\
  neq j} \\sigma_i^2 - \\sigma_j^2}mini\uE020=j\u200B\u03C3i2\u200B\u2212\u03C3j2\u200B\
  1\u200B.\nIn the rectangular case, the gradient will also be numerically unstable\
  \ when\nA has small singular values, as it also depends on the computation of\n\
  1\u03C3i\\frac{1}{\\sigma_i}\u03C3i\u200B1\u200B.\n\n\nSee also\ntorch.linalg.svdvals()\
  \ computes only the singular values.\nUnlike torch.linalg.svd(), the gradients of\
  \ svdvals() are always\nnumerically stable.\ntorch.linalg.eig() for a function that\
  \ computes another type of spectral\ndecomposition of a matrix. The eigendecomposition\
  \ works just on square matrices.\ntorch.linalg.eigh() for a (faster) function that\
  \ computes the eigenvalue decomposition\nfor Hermitian and symmetric matrices.\n\
  torch.linalg.qr() for another (much faster) decomposition that works on general\n\
  matrices.\n\n\nParameters\n\nA (Tensor) \u2013 tensor of shape (*, m, n) where *\
  \ is zero or more batch dimensions.\nfull_matrices (bool, optional) \u2013 controls\
  \ whether to compute the full or reduced\nSVD, and consequently,\nthe shape of the\
  \ returned tensors\nU and Vh. Default: True.\n\n\nKeyword Arguments\n\ndriver (str,\
  \ optional) \u2013 name of the cuSOLVER method to be used. This keyword argument\
  \ only works on CUDA inputs.\nAvailable options are: None, gesvd, gesvdj, and gesvda.\n\
  Default: None.\nout (tuple, optional) \u2013 output tuple of three tensors. Ignored\
  \ if None.\n\n\nReturns\nA named tuple (U, S, Vh) which corresponds to UUU, SSS,\
  \ VHV^{\\text{H}}VH above.\nS will always be real-valued, even when A is complex.\n\
  It will also be ordered in descending order.\nU and Vh will have the same dtype\
  \ as A. The left / right singular vectors will be given by\nthe columns of U and\
  \ the rows of Vh respectively.\n\n\n\nExamples:\n>>> A = torch.randn(5, 3)\n>>>\
  \ U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n>>> U.shape, S.shape, Vh.shape\n\
  (torch.Size([5, 3]), torch.Size([3]), torch.Size([3, 3]))\n>>> torch.dist(A, U @\
  \ torch.diag(S) @ Vh)\ntensor(1.0486e-06)\n\n>>> U, S, Vh = torch.linalg.svd(A)\n\
  >>> U.shape, S.shape, Vh.shape\n(torch.Size([5, 5]), torch.Size([3]), torch.Size([3,\
  \ 3]))\n>>> torch.dist(A, U[:, :3] @ torch.diag(S) @ Vh)\ntensor(1.0486e-06)\n\n\
  >>> A = torch.randn(7, 5, 3)\n>>> U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n\
  >>> torch.dist(A, U @ torch.diag_embed(S) @ Vh)\ntensor(3.0957e-06)\n\n\n"
