api: torch.nn.functional.softmin
doc: "\n\ntorch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)[source]\xB6\
  \nApply a softmin function.\nNote that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x)\
  \ = \\text{Softmax}(-x)Softmin(x)=Softmax(\u2212x). See softmax definition for mathematical\
  \ formula.\nSee Softmin for more details.\n\nParameters\n\ninput (Tensor) \u2013\
  \ input\ndim (int) \u2013 A dimension along which softmin will be computed (so every\
  \ slice\nalong dim will sum to 1).\ndtype (torch.dtype, optional) \u2013 the desired\
  \ data type of returned tensor.\nIf specified, the input tensor is casted to dtype\
  \ before the operation\nis performed. This is useful for preventing data type overflows.\
  \ Default: None.\n\n\nReturn type\nTensor\n\n\n"
