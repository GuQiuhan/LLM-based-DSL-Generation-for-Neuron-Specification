api: torch.linalg.eigh
doc: "\n\ntorch.linalg.eigh(A, UPLO='L', *, out=None)\xB6\nComputes the eigenvalue\
  \ decomposition of a complex Hermitian or real symmetric matrix.\nLetting K\\mathbb{K}K\
  \ be R\\mathbb{R}R or C\\mathbb{C}C,\nthe eigenvalue decomposition of a complex\
  \ Hermitian or real symmetric matrix\nA\u2208Kn\xD7nA \\in \\mathbb{K}^{n \\times\
  \ n}A\u2208Kn\xD7n is defined as\n\nA=Qdiag\u2061(\u039B)QHQ\u2208Kn\xD7n,\u039B\
  \u2208RnA = Q \\operatorname{diag}(\\Lambda) Q^{\\text{H}}\\mathrlap{\\qquad Q \\\
  in \\mathbb{K}^{n \\times n}, \\Lambda \\in \\mathbb{R}^n}A=Qdiag(\u039B)QHQ\u2208\
  Kn\xD7n,\u039B\u2208Rnwhere QHQ^{\\text{H}}QH is the conjugate transpose when QQQ\
  \ is complex, and the transpose when QQQ is real-valued.\nQQQ is orthogonal in the\
  \ real case and unitary in the complex case.\nSupports input of float, double, cfloat\
  \ and cdouble dtypes.\nAlso supports batches of matrices, and if A is a batch of\
  \ matrices then\nthe output has the same batch dimensions.\nA is assumed to be Hermitian\
  \ (resp. symmetric), but this is not checked internally, instead:\n\nIf UPLO= \u2018\
  L\u2019 (default), only the lower triangular part of the matrix is used in the computation.\n\
  If UPLO= \u2018U\u2019, only the upper triangular part of the matrix is used.\n\n\
  The eigenvalues are returned in ascending order.\n\nNote\nWhen inputs are on a CUDA\
  \ device, this function synchronizes that device with the CPU.\n\n\nNote\nThe eigenvalues\
  \ of real symmetric or complex Hermitian matrices are always real.\n\n\nWarning\n\
  The eigenvectors of a symmetric matrix are not unique, nor are they continuous with\n\
  respect to A. Due to this lack of uniqueness, different hardware and\nsoftware may\
  \ compute different eigenvectors.\nThis non-uniqueness is caused by the fact that\
  \ multiplying an eigenvector by\n-1 in the real case or by ei\u03D5,\u03D5\u2208\
  Re^{i \\phi}, \\phi \\in \\mathbb{R}ei\u03D5,\u03D5\u2208R in the complex\ncase\
  \ produces another set of valid eigenvectors of the matrix.\nFor this reason, the\
  \ loss function shall not depend on the phase of the eigenvectors, as\nthis quantity\
  \ is not well-defined.\nThis is checked for complex inputs when computing the gradients\
  \ of this function. As such,\nwhen inputs are complex and are on a CUDA device,\
  \ the computation of the gradients\nof this function synchronizes that device with\
  \ the CPU.\n\n\nWarning\nGradients computed using the eigenvectors tensor will only\
  \ be finite when\nA has distinct eigenvalues.\nFurthermore, if the distance between\
  \ any two eigenvalues is close to zero,\nthe gradient will be numerically unstable,\
  \ as it depends on the eigenvalues\n\u03BBi\\lambda_i\u03BBi\u200B through the computation\
  \ of\n1min\u2061i\u2260j\u03BBi\u2212\u03BBj\\frac{1}{\\min_{i \\neq j} \\lambda_i\
  \ - \\lambda_j}mini\uE020=j\u200B\u03BBi\u200B\u2212\u03BBj\u200B1\u200B.\n\n\n\
  Warning\nUser may see pytorch crashes if running eigh on CUDA devices with CUDA\
  \ versions before 12.1 update 1\nwith large ill-conditioned matrices as inputs.\n\
  Refer to Linear Algebra Numerical Stability for more details.\nIf this is the case,\
  \ user may (1) tune their matrix inputs to be less ill-conditioned,\nor (2) use\
  \ torch.backends.cuda.preferred_linalg_library() to\ntry other supported backends.\n\
  \n\nSee also\ntorch.linalg.eigvalsh() computes only the eigenvalues of a Hermitian\
  \ matrix.\nUnlike torch.linalg.eigh(), the gradients of eigvalsh() are always\n\
  numerically stable.\ntorch.linalg.cholesky() for a different decomposition of a\
  \ Hermitian matrix.\nThe Cholesky decomposition gives less information about the\
  \ matrix but is much faster\nto compute than the eigenvalue decomposition.\ntorch.linalg.eig()\
  \ for a (slower) function that computes the eigenvalue decomposition\nof a not necessarily\
  \ Hermitian square matrix.\ntorch.linalg.svd() for a (slower) function that computes\
  \ the more general SVD\ndecomposition of matrices of any shape.\ntorch.linalg.qr()\
  \ for another (much faster) decomposition that works on general\nmatrices.\n\n\n\
  Parameters\n\nA (Tensor) \u2013 tensor of shape (*, n, n) where * is zero or more\
  \ batch dimensions\nconsisting of symmetric or Hermitian matrices.\nUPLO ('L', 'U',\
  \ optional) \u2013 controls whether to use the upper or lower triangular part\n\
  of A in the computations. Default: \u2018L\u2019.\n\n\nKeyword Arguments\nout (tuple,\
  \ optional) \u2013 output tuple of two tensors. Ignored if None. Default: None.\n\
  \nReturns\nA named tuple (eigenvalues, eigenvectors) which corresponds to \u039B\
  \\Lambda\u039B and QQQ above.\neigenvalues will always be real-valued, even when\
  \ A is complex.\nIt will also be ordered in ascending order.\neigenvectors will\
  \ have the same dtype as A and will contain the eigenvectors as its columns.\n\n\
  \n\n\nExamples::>>> A = torch.randn(2, 2, dtype=torch.complex128)\n>>> A = A + A.T.conj()\
  \  # creates a Hermitian matrix\n>>> A\ntensor([[2.9228+0.0000j, 0.2029-0.0862j],\n\
  \        [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)\n>>> L, Q =\
  \ torch.linalg.eigh(A)\n>>> L\ntensor([0.3277, 2.9415], dtype=torch.float64)\n>>>\
  \ Q\ntensor([[-0.0846+-0.0000j, -0.9964+0.0000j],\n        [ 0.9170+0.3898j, -0.0779-0.0331j]],\
  \ dtype=torch.complex128)\n>>> torch.dist(Q @ torch.diag(L.cdouble()) @ Q.T.conj(),\
  \ A)\ntensor(6.1062e-16, dtype=torch.float64)\n\n\n>>> A = torch.randn(3, 2, 2,\
  \ dtype=torch.float64)\n>>> A = A + A.mT  # creates a batch of symmetric matrices\n\
  >>> L, Q = torch.linalg.eigh(A)\n>>> torch.dist(Q @ torch.diag_embed(L) @ Q.mH,\
  \ A)\ntensor(1.5423e-15, dtype=torch.float64)\n\n\n\n\n"
