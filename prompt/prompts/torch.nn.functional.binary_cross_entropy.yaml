api: torch.nn.functional.binary_cross_entropy
doc: "\n\ntorch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None,\
  \ reduce=None, reduction='mean')[source]\xB6\nMeasure Binary Cross Entropy between\
  \ the target and input probabilities.\nSee BCELoss for details.\n\nParameters\n\n\
  input (Tensor) \u2013 Tensor of arbitrary shape as probabilities.\ntarget (Tensor)\
  \ \u2013 Tensor of the same shape as input with values between 0 and 1.\nweight\
  \ (Tensor, optional) \u2013 a manual rescaling weight\nif provided it\u2019s repeated\
  \ to match input tensor shape\nsize_average (bool, optional) \u2013 Deprecated (see\
  \ reduction). By default,\nthe losses are averaged over each loss element in the\
  \ batch. Note that for\nsome losses, there multiple elements per sample. If the\
  \ field size_average\nis set to False, the losses are instead summed for each minibatch.\
  \ Ignored\nwhen reduce is False. Default: True\nreduce (bool, optional) \u2013 Deprecated\
  \ (see reduction). By default, the\nlosses are averaged or summed over observations\
  \ for each minibatch depending\non size_average. When reduce is False, returns a\
  \ loss per\nbatch element instead and ignores size_average. Default: True\nreduction\
  \ (str, optional) \u2013 Specifies the reduction to apply to the output:\n'none'\
  \ | 'mean' | 'sum'. 'none': no reduction will be applied,\n'mean': the sum of the\
  \ output will be divided by the number of\nelements in the output, 'sum': the output\
  \ will be summed. Note: size_average\nand reduce are in the process of being deprecated,\
  \ and in the meantime,\nspecifying either of those two args will override reduction.\
  \ Default: 'mean'\n\n\nReturn type\nTensor\n\n\nExamples:\n>>> input = torch.randn(3,\
  \ 2, requires_grad=True)\n>>> target = torch.rand(3, 2, requires_grad=False)\n>>>\
  \ loss = F.binary_cross_entropy(torch.sigmoid(input), target)\n>>> loss.backward()\n\
  \n\n"
