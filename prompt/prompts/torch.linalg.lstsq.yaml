api: torch.linalg.lstsq
doc: "\n\ntorch.linalg.lstsq(A, B, rcond=None, *, driver=None)\xB6\nComputes a solution\
  \ to the least squares problem of a system of linear equations.\nLetting K\\mathbb{K}K\
  \ be R\\mathbb{R}R or C\\mathbb{C}C,\nthe least squares problem for a linear system\
  \ AX=BAX = BAX=B with\nA\u2208Km\xD7n,B\u2208Km\xD7kA \\in \\mathbb{K}^{m \\times\
  \ n}, B \\in \\mathbb{K}^{m \\times k}A\u2208Km\xD7n,B\u2208Km\xD7k is defined as\n\
  \nmin\u2061X\u2208Kn\xD7k\u2225AX\u2212B\u2225F\\min_{X \\in \\mathbb{K}^{n \\times\
  \ k}} \\|AX - B\\|_FX\u2208Kn\xD7kmin\u200B\u2225AX\u2212B\u2225F\u200Bwhere \u2225\
  \u2212\u2225F\\|-\\|_F\u2225\u2212\u2225F\u200B denotes the Frobenius norm.\nSupports\
  \ inputs of float, double, cfloat and cdouble dtypes.\nAlso supports batches of\
  \ matrices, and if the inputs are batches of matrices then\nthe output has the same\
  \ batch dimensions.\ndriver chooses the backend function that will be used.\nFor\
  \ CPU inputs the valid values are \u2018gels\u2019, \u2018gelsy\u2019, \u2018gelsd,\
  \ \u2018gelss\u2019.\nTo choose the best driver on CPU consider:\n\nIf A is well-conditioned\
  \ (its condition number is not too large), or you do not mind some precision loss.\n\
  \nFor a general matrix: \u2018gelsy\u2019 (QR with pivoting) (default)\nIf A is\
  \ full-rank: \u2018gels\u2019 (QR)\n\n\nIf A is not well-conditioned.\n\n\u2018\
  gelsd\u2019 (tridiagonal reduction and SVD)\nBut if you run into memory issues:\
  \ \u2018gelss\u2019 (full SVD).\n\n\n\nFor CUDA input, the only valid driver is\
  \ \u2018gels\u2019, which assumes that A is full-rank.\nSee also the full description\
  \ of these drivers\nrcond is used to determine the effective rank of the matrices\
  \ in A\nwhen driver is one of (\u2018gelsy\u2019, \u2018gelsd\u2019, \u2018gelss\u2019\
  ).\nIn this case, if \u03C3i\\sigma_i\u03C3i\u200B are the singular values of A\
  \ in decreasing order,\n\u03C3i\\sigma_i\u03C3i\u200B will be rounded down to zero\
  \ if \u03C3i\u2264rcond\u22C5\u03C31\\sigma_i \\leq \\text{rcond} \\cdot \\sigma_1\u03C3\
  i\u200B\u2264rcond\u22C5\u03C31\u200B.\nIf rcond= None (default), rcond is set to\
  \ the machine precision of the dtype of A times max(m, n).\nThis function returns\
  \ the solution to the problem and some extra information in a named tuple of\nfour\
  \ tensors (solution, residuals, rank, singular_values). For inputs A, B\nof shape\
  \ (*, m, n), (*, m, k) respectively, it contains\n\nsolution: the least squares\
  \ solution. It has shape (*, n, k).\nresiduals: the squared residuals of the solutions,\
  \ that is, \u2225AX\u2212B\u2225F2\\|AX - B\\|_F^2\u2225AX\u2212B\u2225F2\u200B\
  .\nIt has shape equal to the batch dimensions of A.\nIt is computed when m > n and\
  \ every matrix in A is full-rank,\notherwise, it is an empty tensor.\nIf A is a\
  \ batch of matrices and any matrix in the batch is not full rank,\nthen an empty\
  \ tensor is returned. This behavior may change in a future PyTorch release.\nrank:\
  \ tensor of ranks of the matrices in A.\nIt has shape equal to the batch dimensions\
  \ of A.\nIt is computed when driver is one of (\u2018gelsy\u2019, \u2018gelsd\u2019\
  , \u2018gelss\u2019),\notherwise it is an empty tensor.\nsingular_values: tensor\
  \ of singular values of the matrices in A.\nIt has shape (*, min(m, n)).\nIt is\
  \ computed when driver is one of (\u2018gelsd\u2019, \u2018gelss\u2019),\notherwise\
  \ it is an empty tensor.\n\n\nNote\nThis function computes X = A.pinverse() @ B\
  \ in a faster and\nmore numerically stable way than performing the computations\
  \ separately.\n\n\nWarning\nThe default value of rcond may change in a future PyTorch\
  \ release.\nIt is therefore recommended to use a fixed value to avoid potential\n\
  breaking changes.\n\n\nParameters\n\nA (Tensor) \u2013 lhs tensor of shape (*, m,\
  \ n) where * is zero or more batch dimensions.\nB (Tensor) \u2013 rhs tensor of\
  \ shape (*, m, k) where * is zero or more batch dimensions.\nrcond (float, optional)\
  \ \u2013 used to determine the effective rank of A.\nIf rcond= None, rcond is set\
  \ to the machine\nprecision of the dtype of A times max(m, n). Default: None.\n\n\
  \nKeyword Arguments\ndriver (str, optional) \u2013 name of the LAPACK/MAGMA method\
  \ to be used.\nIf None, \u2018gelsy\u2019 is used for CPU inputs and \u2018gels\u2019\
  \ for CUDA inputs.\nDefault: None.\n\nReturns\nA named tuple (solution, residuals,\
  \ rank, singular_values).\n\n\nExamples:\n>>> A = torch.randn(1,3,3)\n>>> A\ntensor([[[-1.0838,\
  \  0.0225,  0.2275],\n     [ 0.2438,  0.3844,  0.5499],\n     [ 0.1175, -0.9102,\
  \  2.0870]]])\n>>> B = torch.randn(2,3,3)\n>>> B\ntensor([[[-0.6772,  0.7758,  0.5109],\n\
  \     [-1.4382,  1.3769,  1.1818],\n     [-0.3450,  0.0806,  0.3967]],\n    [[-1.3994,\
  \ -0.1521, -0.1473],\n     [ 1.9194,  1.0458,  0.6705],\n     [-1.1802, -0.9796,\
  \  1.4086]]])\n>>> X = torch.linalg.lstsq(A, B).solution # A is broadcasted to shape\
  \ (2, 3, 3)\n>>> torch.dist(X, torch.linalg.pinv(A) @ B)\ntensor(1.5152e-06)\n\n\
  >>> S = torch.linalg.lstsq(A, B, driver='gelsd').singular_values\n>>> torch.dist(S,\
  \ torch.linalg.svdvals(A))\ntensor(2.3842e-07)\n\n>>> A[:, 0].zero_()  # Decrease\
  \ the rank of A\n>>> rank = torch.linalg.lstsq(A, B).rank\n>>> rank\ntensor([2])\n\
  \n\n"
